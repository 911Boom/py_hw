{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-04T00:36:17.326169400Z",
     "start_time": "2023-07-04T00:36:17.273897200Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_33788\\2005770231.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;31m# 初始化一个Q表，用来存储每个状态和动作对应的Q值，初始值为0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m \u001B[0mq_table\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mRANGE_NORMAL\u001B[0m \u001B[1;33m**\u001B[0m \u001B[0mNUM_NORMAL\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mRANGE_SPECIAL\u001B[0m \u001B[1;33m**\u001B[0m \u001B[0mNUM_SPECIAL\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRANGE_NORMAL\u001B[0m \u001B[1;33m**\u001B[0m \u001B[0mNUM_NORMAL\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mRANGE_SPECIAL\u001B[0m \u001B[1;33m**\u001B[0m \u001B[0mNUM_SPECIAL\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mq_table\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[1;31m# 开始游戏循环\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    }
   ],
   "source": [
    "# 导入一些需要的库\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 定义一些常量\n",
    "NUM_NORMAL = 6 # 普通数字的个数\n",
    "NUM_SPECIAL = 1 # 特殊数字的个数\n",
    "RANGE_NORMAL = 33 # 普通数字的范围\n",
    "RANGE_SPECIAL = 16 # 特殊数字的范围\n",
    "MAX_EPISODES = 1000 # 最大游戏次数\n",
    "ALPHA = 0.1 # 学习率\n",
    "GAMMA = 0.9 # 折扣因子\n",
    "EPSILON = 0.1 # 探索率\n",
    "\n",
    "# 定义一个函数，用来生成一个随机的答案\n",
    "def generate_answer():\n",
    "    normal_numbers = random.sample(range(1, RANGE_NORMAL + 1), NUM_NORMAL) # 随机选择6个不同的普通数字\n",
    "    special_number = random.randint(1, RANGE_SPECIAL) # 随机选择一个特殊数字\n",
    "    answer = normal_numbers + [special_number] # 合并成一个答案\n",
    "    return answer\n",
    "\n",
    "# 定义一个函数，用来计算两个答案之间的匹配程度，作为奖励\n",
    "def calculate_reward(answer, guess):\n",
    "    normal_match = len(set(answer[:-1]) & set(guess[:-1])) # 计算普通数字的匹配个数\n",
    "    special_match = int(answer[-1] == guess[-1]) # 计算特殊数字是否匹配\n",
    "    reward = normal_match + special_match * 2 # 计算总的奖励，特殊数字匹配得分更高\n",
    "    return reward\n",
    "\n",
    "# 定义一个函数，用来根据Q表选择一个动作，即一个猜测的答案\n",
    "def choose_action(state, q_table):\n",
    "    if np.random.uniform() < EPSILON: # 以一定的概率进行探索，随机选择一个动作\n",
    "        action = generate_answer()\n",
    "    else: # 以一定的概率进行利用，选择Q值最大的动作\n",
    "        action = q_table[state].argmax()\n",
    "    return action\n",
    "\n",
    "# 定义一个函数，用来更新Q表\n",
    "def update_q_table(state, action, reward, next_state, q_table):\n",
    "    best_q = q_table[next_state].max() # 找到下一个状态对应的最大Q值\n",
    "    q_table[state, action] += ALPHA * (reward + GAMMA * best_q - q_table[state, action]) # 根据贝尔曼方程更新Q值\n",
    "\n",
    "# 初始化一个Q表，用来存储每个状态和动作对应的Q值，初始值为0\n",
    "q_table = np.zeros((RANGE_NORMAL ** NUM_NORMAL * RANGE_SPECIAL ** NUM_SPECIAL, RANGE_NORMAL ** NUM_NORMAL * RANGE_SPECIAL ** NUM_SPECIAL))\n",
    "print(q_table)\n",
    "# 开始游戏循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: The answer is [15, 18, 25, 5, 20, 31, 14]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'q_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_33788\\1121386932.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mchoose_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mq_table\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# 根据Q表选择一个动作，即一个猜测的答案\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m         \u001B[0mreward\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalculate_reward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# 计算当前动作对应的奖励，即匹配程度\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0mnext_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;31m# 更新下一个状态，即增加一次猜测次数\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'q_table' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_EPISODES):\n",
    "    answer = generate_answer() # 生成一个随机的答案\n",
    "    state = 0 # 初始化状态为0，表示没有任何猜测过程\n",
    "    done = False # 初始化结束标志为False\n",
    "    print(f\"Episode {i+1}: The answer is {answer}\") # 打印当前回合的答案\n",
    "    while not done:\n",
    "        action = choose_action(state, q_table) # 根据Q表选择一个动作，即一个猜测的答案\n",
    "        reward = calculate_reward(answer, action) # 计算当前动作对应的奖励，即匹配程度\n",
    "        next_state = state + 1 # 更新下一个状态，即增加一次猜测次数\n",
    "        update_q_table(state, action, reward, next_state, q_table) # 更新Q表\n",
    "        print(f\"Guess {next_state}: {action}, reward: {reward}\") # 打印当前的猜测和奖励\n",
    "        state = next_state # 更新状态\n",
    "        if reward == NUM_NORMAL + NUM_SPECIAL * 2: # 如果奖励等于最大值，即完全匹配，结束游戏\n",
    "            done = True\n",
    "            print(\"You win!\")\n",
    "        elif state == RANGE_NORMAL ** NUM_NORMAL * RANGE_SPECIAL ** NUM_SPECIAL: # 如果状态等于最大值，即猜测次数用完，结束游戏\n",
    "            done = True\n",
    "            print(\"You lose!\")\n",
    "    print(\"End of episode\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T00:36:07.079814600Z",
     "start_time": "2023-07-04T00:36:07.038408200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
